{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import pca_lowrank\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "openPMDBuffer = Queue(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from inSituML.ks_producer_openPMD_streaming import StreamLoader\n",
    "from inSituML.ks_transform_policies import AbsoluteSquare, BoxesAttributesParticles\n",
    "\n",
    "normalization_values = dict(\n",
    "    momentum_mean = 1.2091940752668797e-08,\n",
    "    momentum_std = 0.11923234769525472,\n",
    "    force_mean = -2.7682006649827533e-09,\n",
    "    force_std = 7.705477610810592e-05\n",
    ")\n",
    "\n",
    "\n",
    "streamLoader_config = dict(\n",
    "    t0 = 900,\n",
    "    t1 = 998,\n",
    "    # t0 =  1800,\n",
    "    # t1 = 1810,\n",
    "    streaming_config = None,\n",
    "    #pathpattern1 = \"/bigdata/hplsim/aipp/SC24_PIConGPU-Continual-Learning/03-30_learning-rate-scaling-with-ranks_chamfersdistance_fix-gpu-volume/24-nodes_full-picongpu-data/simOutput/openPMD/simData_%T.bp\", # files on hemera\n",
    "    #pathpattern2 = \"/bigdata/hplsim/aipp/SC24_PIConGPU-Continual-Learning/03-30_learning-rate-scaling-with-ranks_chamfersdistance_fix-gpu-volume/24-nodes_full-picongpu-data/simOutput/radiationOpenPMD/e_radAmplitudes%T.bp\", # files on hemera\n",
    "    #pathpattern1 = \"/bigdata/hplsim/production/KHI_for_GB_MR/runs/014_KHI_007_noWindowFunction/simOutput/openPMD/simData_%T.bp\", # files on hemera\n",
    "    #pathpattern2 = \"/bigdata/hplsim/production/KHI_for_GB_MR/runs/014_KHI_007_noWindowFunction/simOutput/radiationOpenPMD/e_radAmplitudes%T.bp\", # files on hemera\n",
    "    #pathpattern1 = \"/bigdata/hplsim/production/KHI_for_GB_MR/runs/015_KHI_009_noWindowFunction/simOutput/openPMD/simData_%T.bp\", # files on hemera\n",
    "    #pathpattern2 = \"/bigdata/hplsim/production/KHI_for_GB_MR/runs/015_KHI_009_noWindowFunction/simOutput/radiationOpenPMD/e_radAmplitudes%T.bp\", # files on hemera\n",
    "    particle_pathpattern = \"/bigdata/hplsim/aipp/SC24_PIConGPU-Continual-Learning/24-nodes_full-picongpu-data/04-01_1013/simOutput/openPMD/simData_%T.bp5\",\n",
    "    radiation_pathpattern = \"/bigdata/hplsim/aipp/SC24_PIConGPU-Continual-Learning/24-nodes_full-picongpu-data/04-01_1013/simOutput/radiationOpenPMD/e_radAmplitudes_%T.bp5\",\n",
    "    \n",
    "    amplitude_direction=0, # choose single direction along which the radiation signal is observed, max: N_observer-1, where N_observer is defined in PIConGPU's radiation plugin\n",
    "    phase_space_variables = [\"momentum\", \"force\"], # allowed are \"position\", \"momentum\", and \"force\". If \"force\" is set, \"momentum\" needs to be set too.\n",
    "    number_particles_per_gpu = 30000,\n",
    "    verbose=False,\n",
    "    ## offline training params\n",
    "    num_epochs = .01, #.0625\n",
    "    normalization = normalization_values\n",
    ")\n",
    "\n",
    "timeBatchLoader = StreamLoader(openPMDBuffer, \n",
    "                                        streamLoader_config,\n",
    "                                        BoxesAttributesParticles(), AbsoluteSquare())\n",
    "\n",
    "timeBatchLoader.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openPMDBuffer.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(0, 100, 10):\n",
    "    data.append(openPMDBuffer.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import share.configs.model_config as model_config\n",
    "import share.configs.io_config_hemera as io_config\n",
    "\n",
    "config = model_config.config\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from inSituML.ks_models import INNModel\n",
    "\n",
    "from inSituML.utilities import MMD_multiscale, fit, load_checkpoint\n",
    "from inSituML.args_transform import MAPPING_TO_LOSS\n",
    "from inSituML.encoder_decoder import Encoder\n",
    "from inSituML.encoder_decoder import Encoder\n",
    "from inSituML.encoder_decoder import Conv3DDecoder, MLPDecoder\n",
    "from inSituML.loss_functions import EarthMoversLoss\n",
    "from inSituML.networks import VAE, ConvAutoencoder\n",
    "\n",
    "world_size = 1\n",
    "\n",
    "class ModelFinal(nn.Module):\n",
    "    def __init__(self,\n",
    "                base_network,\n",
    "                inner_model,\n",
    "                loss_function_IM = None,\n",
    "                weight_AE=1.0,\n",
    "                weight_IM=1.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_network = base_network\n",
    "        self.inner_model = inner_model\n",
    "        self.loss_function_IM = loss_function_IM\n",
    "        self.weight_AE = weight_AE\n",
    "        self.weight_IM = weight_IM\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        loss_AE,loss_ae_reconst,kl_loss, _, encoded = self.base_network(x)\n",
    "\n",
    "        # Check if the inner model is an instance of INNModel\n",
    "        if isinstance(self.inner_model, INNModel):\n",
    "            # Use the compute_losses function of INNModel\n",
    "            loss_IM, l_fit,l_latent,l_rev = self.inner_model.compute_losses(encoded, y)\n",
    "            total_loss = loss_AE*self.weight_AE + loss_IM*self.weight_IM\n",
    "\n",
    "            losses = {\n",
    "                'total_loss': total_loss,\n",
    "                'loss_AE': loss_AE*self.weight_AE,\n",
    "                'loss_IM': loss_IM*self.weight_IM,\n",
    "                'loss_ae_reconst': loss_ae_reconst,\n",
    "                'kl_loss': kl_loss,\n",
    "                'l_fit': l_fit,\n",
    "                'l_latent': l_latent,\n",
    "                'l_rev': l_rev,\n",
    "                    }\n",
    "\n",
    "            return losses\n",
    "        else:\n",
    "            # For other types of models, such as MAF\n",
    "            loss_IM = self.inner_model(inputs=encoded, context=y)\n",
    "            total_loss = loss_AE*self.weight_AE + loss_IM * self.weight_IM\n",
    "\n",
    "            losses = {\n",
    "                'total_loss': total_loss,\n",
    "                'loss_AE': loss_AE*self.weight_AE,\n",
    "                'loss_IM': loss_IM*self.weight_IM,\n",
    "                'loss_ae_reconst': loss_ae_reconst,\n",
    "                'kl_loss': kl_loss\n",
    "                    }\n",
    "\n",
    "            return losses\n",
    "\n",
    "    def reconstruct(self,x, y, num_samples = 1):\n",
    "\n",
    "        if isinstance(self.inner_model, INNModel):\n",
    "            lat_z_pred = self.inner_model(x, y, rev = True)\n",
    "            y = self.base_network.decoder(lat_z_pred)\n",
    "        else:\n",
    "            lat_z_pred = self.inner_model.sample_pointcloud(num_samples = num_samples, cond=y)\n",
    "            y = self.base_network.decoder(lat_z_pred)\n",
    "\n",
    "        return y, lat_z_pred\n",
    "\n",
    "\n",
    "VAE_encoder_kwargs = {\"ae_config\":\"non_deterministic\",\n",
    "                \"z_dim\":model_config.latent_space_dims,\n",
    "                \"input_dim\":io_config.ps_dims,\n",
    "                \"conv_layer_config\":[16, 32, 64, 128, 256, 608],\n",
    "                \"conv_add_bn\": False,\n",
    "                \"fc_layer_config\":[544]}\n",
    "\n",
    "VAE_decoder_kwargs = {\"z_dim\":model_config.latent_space_dims,\n",
    "                \"input_dim\":io_config.ps_dims,\n",
    "                \"initial_conv3d_size\":[16, 4, 4, 4],\n",
    "                \"add_batch_normalisation\":False,\n",
    "                    \"fc_layer_config\":[1024]}\n",
    "def load_objects(rank):\n",
    "\n",
    "    torch.cuda.set_device(rank)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    loss_fn_for_VAE = MAPPING_TO_LOSS[model_config.config['loss_function']](**model_config.config['loss_kwargs'])\n",
    "\n",
    "    VAE_obj = VAE(encoder = Encoder,\n",
    "            encoder_kwargs = VAE_encoder_kwargs,\n",
    "            decoder = Conv3DDecoder,\n",
    "            z_dim=model_config.latent_space_dims,\n",
    "            decoder_kwargs = VAE_decoder_kwargs,\n",
    "            loss_function = loss_fn_for_VAE,\n",
    "            property_=\"momentum_force\",\n",
    "            particles_to_sample = io_config.number_of_particles,\n",
    "            ae_config=\"non_deterministic\",\n",
    "            use_encoding_in_decoder=False,\n",
    "            weight_kl=model_config.config[\"lambd_kl\"],\n",
    "            device=rank)\n",
    "\n",
    "    # conv_AE\n",
    "#     conv_AE_encoder_kwargs = {\"ae_config\":\"simple\",\n",
    "#                     \"z_dim\":model_config.latent_space_dims,\n",
    "#                     \"input_dim\":io_config.ps_dims,\n",
    "#                     \"conv_layer_config\":[16, 32, 64, 128, 256, 512],\n",
    "#                     \"conv_add_bn\": False}\n",
    "\n",
    "#     conv_AE_decoder_kwargs = {\"z_dim\":model_config.latent_space_dims,\n",
    "#                     \"input_dim\":io_config.ps_dims,\n",
    "#                     \"add_batch_normalisation\":False}\n",
    "\n",
    "#     conv_AE = ConvAutoencoder(encoder = Encoder,\n",
    "#                             encoder_kwargs = conv_AE_encoder_kwargs,\n",
    "#                             decoder = Conv3DDecoder,\n",
    "#                             decoder_kwargs = conv_AE_decoder_kwargs,\n",
    "#                             loss_function = EarthMoversLoss(),\n",
    "#                             )\n",
    "\n",
    "    # MAF inner model (not used in final runs)\n",
    "    # inner_model = PC_MAF(dim_condition=config[\"dim_condition\"],\n",
    "    #                         dim_input=config[\"dim_input\"],\n",
    "    #                         num_coupling_layers=config[\"num_coupling_layers\"],\n",
    "    #                         hidden_size=config[\"hidden_size\"],\n",
    "    #                         device=rank,\n",
    "    #                         num_blocks_mat = config[\"num_blocks_mat\"],\n",
    "    #                         activation = config[\"activation\"]\n",
    "    #                         )\n",
    "\n",
    "    # INN\n",
    "    inner_model = INNModel(ndim_tot=config[\"ndim_tot\"],\n",
    "                    ndim_x=config[\"ndim_x\"],\n",
    "                    ndim_y=config[\"ndim_y\"],\n",
    "                    ndim_z=config[\"ndim_z\"],\n",
    "                    loss_fit=fit,\n",
    "                    loss_latent=MMD_multiscale,\n",
    "                    loss_backward=MMD_multiscale,\n",
    "                    lambd_predict=config[\"lambd_predict\"],\n",
    "                    lambd_latent=config[\"lambd_latent\"],\n",
    "                    lambd_rev=config[\"lambd_rev\"],\n",
    "                    zeros_noise_scale=config[\"zeros_noise_scale\"],\n",
    "                    y_noise_scale=config[\"y_noise_scale\"],\n",
    "                    hidden_size=config[\"hidden_size\"],\n",
    "                    activation=config[\"activation\"],\n",
    "                    num_coupling_layers=config[\"num_coupling_layers\"],\n",
    "                    device = rank)\n",
    "\n",
    "    #model = ModelFinal(VAE_obj, inner_model, EarthMoversLoss())\n",
    "    #model = ModelFinal(conv_AE, inner_model, EarthMoversLoss())\n",
    "    model = ModelFinal(VAE_obj,\n",
    "                       inner_model,\n",
    "                       EarthMoversLoss(),\n",
    "                       weight_AE=config[\"lambd_AE\"],\n",
    "                       weight_IM=config[\"lambd_IM\"])\n",
    "\n",
    "\n",
    "    #Load a pre-trained model\n",
    "   \n",
    "    #map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n",
    "    \n",
    "    # updated_state_dict = {key.replace('VAE.', 'base_network.'): value for key, value in original_state_dict.items()}\n",
    "    updated_state_dict = {key.replace('module.', ''): value for key, value in ckpt[\"model\"].items()}\n",
    "    model.load_state_dict(updated_state_dict)\n",
    "\n",
    "    lr = config[\"lr\"]\n",
    "    bs_factor = io_config.trainBatchBuffer_config[\"training_bs\"] / 2 * world_size\n",
    "    lr = lr * config[\"lr_scaling\"](bs_factor)\n",
    "    print(\"Skaling learning rate from {} to {} due to bs factor {}\".format(config[\"lr\"], lr, bs_factor))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=config[\"betas\"],\n",
    "                         eps=config[\"eps\"], weight_decay=config[\"weight_decay\"])\n",
    "    if ( \"lr_annealingRate\" not in config ) or config[\"lr_annealingRate\"] is None:\n",
    "        scheduler = None\n",
    "    else:\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=config[\"lr_annealingRate\"])\n",
    "\n",
    "    return optimizer, scheduler, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,100,30):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "boxlist = []\n",
    "for i in range(90):\n",
    "    if data[5][1][i][410] >-15 and data[5][1][i][210] >-5:\n",
    "        plt.plot(data[5][1][i], label=str(i))\n",
    "        boxlist.append(i)\n",
    "plt.legend()\n",
    "#plt.xlim(400,430)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckptfn = \"/home/kelling/checkout/FWKT/InSituML/main/ModelHelpers/cINN/slurm-6921762/model_8081\"\n",
    "#ckptfn = \"/home/kelling/checkout/FWKT/InSituML/main/ModelHelpers/cINN/slurm-6921753/model_809\"\n",
    "chkptfn = \"/home/kelling/checkout/FWKT/InSituML/main/ModelHelpers/cINN/runs_y/slurm-6921781/model_19000\"\n",
    "chkptfn = \"/home/kelling/checkout/FWKT/InSituML/main/ModelHelpers/cINN/runs_y/slurm-6921781/model_19000\"\n",
    "#ckptfn = \"/home/kelling/checkout/FWKT/InSituML/main/ModelHelpers/cINN/slurm-6921842/model_8081\"\n",
    "#ckptfn = \"/home/kelling/checkout/FWKT/InSituML/main/ModelHelpers/cINN/trained_models/inn_vae_latent_544_sim014_859eopan/model_950\"\n",
    "#ckptfn = \"/home/kelling/checkout/FWKT/InSituML/main/ModelHelpers/cINN/runs014_30k/slurm-{}/model_{}\"\n",
    "#ckptfn = chkptfn.format(6921756, 809) # 950, contniue at 150k\n",
    "#ckptfn = chkptfn.format(6921771, 8081) # 950, tuned at 30k\n",
    "chkptfn = \"/home/kelling/checkout/FWKT/InSituML/main/ModelHelpers/cINN/slurm-{}/model_{}\"\n",
    "#chkptfn = chkptfn.format(6923061, 800) # 150, tuned at 30k, continual_bs=4, buffersize=5,training_bs=4, offline\n",
    "#chkptfn = chkptfn.format(6923062, 800) # 150, tuned at 30k, continual_bs=0, buffersize=4,training_bs=4, offline\n",
    "#chkptfn = chkptfn.format(6923051, 1600) # 950, tuned at 30k, Y, continual_bs=4, buffersize=5,training_bs=4, offline\n",
    "#chkptfn = chkptfn.format(6923053, 2000) # 950, tuned at 30k, continual_bs=4, buffersize=5,training_bs=4, offline\n",
    "#chkptfn = chkptfn.format(6923281, 800) # None, continual_bs=4, buffersize=5,training_bs=4, offline\n",
    "#chkptfn = chkptfn.format(6923976, 12000) # chamfers streaming, rep 6, lr.001, Y\n",
    "#chkptfn = chkptfn.format(6923987, 25600) # chamfers streaming, rep 8, lr.0001, Y\n",
    "#chkptfn = chkptfn.format(6924125, 21600) # chamfers streaming, rep 8, lr.001, Y\n",
    "#chkptfn = chkptfn.format(6924126, 21600) # chamfers streaming, rep 8, lr.001, Y\n",
    "\n",
    "#chkptfn = \"/bigdata/hplsim/scratch/kelling/chamfers/slurm-{}/model_{}\"\n",
    "#chkptfn = chkptfn.format(6923899, 5600) # chamfers straming, rep 0\n",
    "#chkptfn = chkptfn.format(6923925, 24000) # chamfers straming, rep 4 \"red curve\"\n",
    "\n",
    "\n",
    "\n",
    "#chkptfn = \"/bigdata/hplsim/production/steinigk/008-nodes_lr-0.0005_verbose/simOutput/model_376\"\n",
    "#chkptfn = \"/bigdata/hplsim/aipp/SC24_PIConGPU-Continual-Learning/03-30_learning-rate-scaling-with-ranks_chamfersdistance_fix-gpu-volume/48-nodes_lr-0.0001_min-tb-16/simOutput/model_1479\"\n",
    "chkptfn = \"/bigdata/hplsim/aipp/SC24_PIConGPU-Continual-Learning/03-30_learning-rate-scaling-with-ranks_chamfersdistance_fix-gpu-volume/24-nodes_lr-0.0001_min-tb-16/simOutput/model_1479\"\n",
    "#chkptfn = \"/bigdata/hplsim/aipp/SC24_PIConGPU-Continual-Learning/03-30_learning-rate-scaling-with-ranks_chamfersdistance_fix-gpu-volume/{}-nodes_lr-{}_min-tb-{}/simOutput/model_{}\"\n",
    "#chkptfn = chkptfn.format(96,\"0.0005\", 16, 1400) \n",
    "chkptfn = \"/bigdata/hplsim/aipp/SC24_PIConGPU-Continual-Learning/04-01_rerun-independent-AE-scaling_chamfersdistance_fix-gpu-volume_scaling/8-nodes_lr-0.0001_min-tb-4_lrAE-20/04-01_1645/simOutput/model_350\"\n",
    "chkptfn = \"/bigdata/hplsim/aipp/SC24_PIConGPU-Continual-Learning/04-02_single-gpu-offline-training-from-24-node_hemera/trainingOutput/model_9000\"\n",
    "\n",
    "chkptfn = \"/bigdata/hplsim/aipp/SC24_PIConGPU-Continual-Learning/parallelMMD/slurm-{}/model_{}\"\n",
    "#chkptfn = chkptfn.format(7778146, 4000)\n",
    "#chkptfn = chkptfn.format(7778301, 5600)\n",
    "#chkptfn = chkptfn.format(7779695, 2400)\n",
    "chkptfn = chkptfn.format(7779773, 10400)\n",
    "#chkptfn = chkptfn.format(7779780, 1600)\n",
    "#chkptfn = chkptfn.format(7784141, 4000)\n",
    "\n",
    "\n",
    "print(chkptfn)\n",
    "ckpt = torch.load(chkptfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt, sched, model = load_objects(0)\n",
    "model.cuda()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(boxList, xlim=(-3,3), ylim=(-.4,.4), centerHistSamples=0, lbllist=None):\n",
    "    plt.rcParams.update({'font.size': 30})\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for it, tt in enumerate(data[-4:]):\n",
    "            x = tt[0][boxList]\n",
    "            y = tt[1][boxList]\n",
    "            dec = model.base_network.forward(x.transpose(1,2).cuda())\n",
    "            \n",
    "            zpca = pca_lowrank(dec[4])\n",
    "            inz = torch.matmul(dec[4], zpca[-1][:, :2])\n",
    "            inz = inz.detach().cpu()\n",
    "            \n",
    "            lat_z = dec[4].detach().cpu()\n",
    "            decoded = dec[3].detach().cpu()\n",
    "            pc_pr = []\n",
    "            lat_z_plot = []\n",
    "            centerDist = []\n",
    "            \n",
    "            for i in range(max(centerHistSamples, 4)):\n",
    "                p, l = model.reconstruct(x.cuda(), y.cuda())\n",
    "                center = torch.mean(p[:,:,0], axis=1)\n",
    "                \n",
    "                l = torch.matmul(l, zpca[-1][:, :2])\n",
    "                \n",
    "                if i < 3 or centerHistSamples == 0:\n",
    "                    pc_pr.append(p.detach().cpu().numpy())\n",
    "                if centerHistSamples > 0:\n",
    "                    centerDist.append(center.detach().cpu().numpy())\n",
    "                    \n",
    "                lat_z_plot.append(l.detach().cpu())\n",
    "                \n",
    "            lat_z_plot = np.stack(lat_z_plot)\n",
    "            #print(lat_z_plot.shape)\n",
    "\n",
    "            if centerHistSamples>0:\n",
    "                centerDist = np.stack(centerDist)\n",
    "            for bi in range(len(boxList)):\n",
    "                \n",
    "                rgen = []\n",
    "                \n",
    "                numPlotCol = 2+len(pc_pr)+(centerHistSamples>0)+1\n",
    "                fig, ax = plt.subplots(1, numPlotCol, figsize=(10*numPlotCol, 10), squeeze=True)\n",
    "                for i, p in enumerate(pc_pr):\n",
    "                    rgen.append(p[bi,:,0])\n",
    "                    ax[2+i].hexbin(p[bi,:,0], p[bi,:,1], bins=\"log\")\n",
    "                    ax[2+i].set(xlim=xlim, ylim=ylim, title=\"INN bw sample\")\n",
    "                    ax[2+i].sharey(ax[0])\n",
    "                    \n",
    "                if centerHistSamples>0:\n",
    "                    chist, binbounds = np.histogram(centerDist[:, bi])\n",
    "                    ax[-2].bar((binbounds[1:]+binbounds[:-1])/2, chist)\n",
    "                    ax[-2].set(xlim=xlim, title=\"Histogram\")\n",
    "                    \n",
    "                ax[-1].scatter(lat_z_plot[:, bi, 0], lat_z_plot[:, bi, 0], c=\"gray\", s=500)\n",
    "                ax[-1].scatter(inz[:,0], inz[:,1], c=lbllist, marker=\"+\", s=1000)\n",
    "                \n",
    "                ax[0].hexbin(x[bi,0,:].numpy(), x[bi,1,:].numpy(), bins=\"log\")\n",
    "                ax[0].set(xlim=xlim, ylim=ylim, title=\"GT@{} box {}\".format(it, boxList[bi]))\n",
    "                ax[1].hexbin(decoded[bi,:,0].numpy(), decoded[bi,:,1].numpy(), bins=\"log\")\n",
    "                ax[1].set(xlim=xlim, ylim=ylim, title=\"AE\")\n",
    "                ax[1].sharey(ax[0])\n",
    "                \n",
    "                results.append([(it, bi), rgen, x[bi,0,:].numpy()])\n",
    "\n",
    "                \n",
    "                fig.show()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "boxlist = [60,61,81,0,1,2,3,4,5,87,88,89]\n",
    "lbllist = [0,0,0,1,1,1,2,2,2,3,3,3]\n",
    "#boxlist = [87,88,89]\n",
    "part = plot(boxList = boxlist, xlim=(-3,3), ylim=(-.1,.1), centerHistSamples=10, lbllist=lbllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def center(bins):\n",
    "    return ( bins[1:]+bins[:-1])/2\n",
    "\n",
    "def momDenorm(x):\n",
    "    return x * normalization_values[\"momentum_std\"] + normalization_values[\"momentum_mean\"]\n",
    "\n",
    "i = -1\n",
    "gthist, gtbins = np.histogram(momDenorm(part[i][2]))\n",
    "gtx = center(gtbins)\n",
    "genhist, genbins = np.histogram(momDenorm(part[i][1][2]))\n",
    "print(part[i][0])\n",
    "genx = center(genbins)\n",
    "plt.plot(gtx, gthist)\n",
    "plt.plot(genx, gthist)\n",
    "\n",
    "pickPxGT = momDenorm(part[i][2])\n",
    "pickPxGen = momDenorm(part[i][1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRad(boxList, xlim=(-3,3), ylim=(-.4,.4), centerHistSamples=0):\n",
    "    plt.rcParams.update({'font.size': 30})\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for it, tt in enumerate(data[-4:]):\n",
    "            x = tt[0][boxList]\n",
    "            y = tt[1][boxList]\n",
    "            pc_pr = []\n",
    "            lat_z_pred = []\n",
    "            centerDist = []\n",
    "            \n",
    "            ae = model.base_network.forward(x.transpose(1,2).cuda())\n",
    "            rad = model.inner_model.forward(ae[4])\n",
    "            loss_IM, l_fit,l_latent,l_rev = model.inner_model.compute_losses(ae[4], y)\n",
    "            \n",
    "            rad = rad.detach().cpu().numpy()\n",
    "\n",
    "            numPlotCol = rad.shape[0]\n",
    "            fig, ax = plt.subplots(1, numPlotCol, figsize=(10*numPlotCol, 10), squeeze=True, sharey=True)\n",
    "            for bi in range(len(boxList)):\n",
    "\n",
    "                ax[bi].set_title(\"l_fit={:0.5} b={}\".format(l_fit.item(), boxList[bi]))\n",
    "                results.append([(it, bi), rad[bi, -512:], y[bi]])\n",
    "                ax[bi].plot(rad[bi, -512:])\n",
    "                ax[bi].plot(y[bi])\n",
    "                    \n",
    "            fig.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rad = plotRad(boxList = boxlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rad[-1][1])\n",
    "plt.plot(rad[-1][2])\n",
    "\n",
    "pickRadGT = rad[-1][1]\n",
    "pickRadPred = rad[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"picks.npz\", pickPxGen=pickPxGen, pickPxGT=pickPxGT, pickRadPred=pickRadPred, pickRadGT=pickRadGT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plotRad(boxList = [1,2,5,6,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rad014fn = \"/bigdata/hplsim/production/KHI_for_GB_MR/runs/014_KHI_007_noWindowFunction/simOutput/totalRad/e_radiation_{}.dat\"\n",
    "radFfn =\"/bigdata/hplsim/aipp/SC24_PIConGPU-Continual-Learning/03-30_learning-rate-scaling-with-ranks_chamfersdistance_fix-gpu-volume/{}-nodes_lr-0.001_min-tb-{}/simOutput/totalRad/e_radiation_{}.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 900\n",
    "tF = t\n",
    "obs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "fn = rad014fn.format(t)\n",
    "rad014 = np.loadtxt(fn)\n",
    "plt.plot(np.log(rad014[obs]), linestyle=\"-\", label=\"014\")\n",
    "for nn in (8,):\n",
    "    for tb in (4,8,16):\n",
    "        fn = radFfn.format(nn, tb, tF)\n",
    "        #print(fn)\n",
    "        radF = np.loadtxt(fn)\n",
    "        plt.plot(np.log(radF[obs]), linestyle=\":\", label=\"Frontier {} nodes\".format(nn))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.load(\"/bigdata/hplsim/aipp/SC24_PIConGPU-Continual-Learning/03-30_learning-rate-scaling-with-ranks_chamfersdistance_fix-gpu-volume/96-nodes_lr-0.0001_min-tb-16/simOutput/streamedRadiation/ts_1.npy\")\n",
    "\n",
    "#r = np.load(\"/home/kelling/checkout/FWKT/InSituML/main/ModelHelpers/cINN/slurm-6924589/ts_1.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.ylim((-1e2,0))\n",
    "#plt.xlim((280,400))\n",
    "print(r.shape)\n",
    "plt.plot(np.log(-r[0,0,:150]))\n",
    "#plt.plot(r[0,0,:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caarenv",
   "language": "python",
   "name": "caarenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
