{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d99a75-44b0-4cde-9cec-68f48980f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nflows\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.transforms.standard import AffineTransform\n",
    "from nflows.transforms.permutations import ReversePermutation\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n",
    "from nflows.transforms.base import CompositeTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3252f2ce-64b9-42f9-bd1d-90db6689e228",
   "metadata": {},
   "source": [
    "## Define the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636bd5d7-f2e2-40ab-a4c6-e2af4a38ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PC_MAF(nn.Module):\n",
    "    def __init__(self, \n",
    "                 dim_condition,\n",
    "                 dim_input,\n",
    "                 num_coupling_layers=1,\n",
    "                 hidden_size=128,\n",
    "                 device='cpu',\n",
    "                 weight_particles=False,\n",
    "                 num_blocks_mat = 2,     \n",
    "                 activation = 'relu',\n",
    "                 random_mask = False):\n",
    "        \n",
    "        '''\n",
    "        Masked autoregressive flows model from https://papers.nips.cc/paper/2017/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html\n",
    "        Args:\n",
    "            dim_condition(integer): dimensionality of condition\n",
    "            dim_input(integer): dimensionality of input\n",
    "            num_coupling_blocks(integer): number of coupling blocks in the model\n",
    "            hidden_size(integer): number of hidden units per hidden layer in subnetworks\n",
    "            device: \"cpu\" or \"cuda\"\n",
    "\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.num_coupling_layers = num_coupling_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_condition = dim_condition\n",
    "        \n",
    "        self.num_blocks_mat = num_blocks_mat\n",
    "        self.random_mask = random_mask\n",
    "        \n",
    "        \n",
    "        # Activation functions\n",
    "        activation_functions = {\n",
    "            'relu': F.relu,\n",
    "            'sigmoid': torch.sigmoid,\n",
    "            'tanh': torch.tanh,\n",
    "            'elu': F.elu,\n",
    "            'silu': F.silu,\n",
    "            'leaky_relu': F.leaky_relu,\n",
    "            'gelu': F.gelu\n",
    "        }\n",
    "        self.activation = activation_functions.get(activation)\n",
    "        \n",
    "        if self.activation is None:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "            \n",
    "        self.model = self.init_model().to(self.device)\n",
    "        self.weight_particles = weight_particles\n",
    "    \n",
    "    def init_model(self):\n",
    "        base_dist = nflows.distributions.normal.StandardNormal(shape=[self.dim_input])\n",
    "        \n",
    "        transforms = []\n",
    "        for _ in range(self.num_coupling_layers):\n",
    "            transforms.append(ReversePermutation(features=self.dim_input))\n",
    "            transforms.append(MaskedAffineAutoregressiveTransform(features=self.dim_input, \n",
    "                                                                  hidden_features=self.hidden_size, \n",
    "                                                                  context_features=self.dim_condition,\n",
    "                                                                  use_residual_blocks=True,  \n",
    "                                                                  num_blocks = self.num_blocks_mat,\n",
    "                                                                  activation = self.activation,\n",
    "                                                                  random_mask = self.random_mask))\n",
    "        transform = CompositeTransform(transforms)\n",
    "\n",
    "        return Flow(transform, base_dist).to(self.device)\n",
    "\n",
    "    def forward(self, x, p):\n",
    "        loss = self.model(x, c=p)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03ed44-5c48-4aa8-a891-a8d86d504ef0",
   "metadata": {},
   "source": [
    "## Functions for normalization, sampling, plotting, and saving checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6c2e3-2277-46ba-a7da-cd6794b1024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(original_array):\n",
    "    xyz_columns = original_array[:, :3]\n",
    "    x_min, x_max = xyz_columns[:, 0].min(), xyz_columns[:, 0].max()\n",
    "    y_min, y_max = xyz_columns[:, 1].min(), xyz_columns[:, 1].max()\n",
    "    z_min, z_max = xyz_columns[:, 2].min(), xyz_columns[:, 2].max()\n",
    "\n",
    "    xyz_columns[:, 0] = (xyz_columns[:, 0] - x_min) / (x_max - x_min)\n",
    "    xyz_columns[:, 1] = (xyz_columns[:, 1] - y_min) / (y_max - y_min)\n",
    "    xyz_columns[:, 2] = (xyz_columns[:, 2] - z_min) / (z_max - z_min)\n",
    "\n",
    "    normalized_array = np.concatenate((xyz_columns, original_array[:, 3:]), axis=1)\n",
    "    return normalized_array\n",
    "\n",
    "def denormalize_columns(normalized_array, gt):\n",
    "    \n",
    "    x_min, x_max = gt[:, 0].min(), gt[:, 0].max()\n",
    "    y_min, y_max = gt[:, 1].min(), gt[:, 1].max()\n",
    "    z_min, z_max = gt[:, 2].min(), gt[:, 2].max()\n",
    "\n",
    "    xyz_columns = normalized_array[:, :3]\n",
    "\n",
    "    xyz_columns[:, 0] = xyz_columns[:, 0] * (x_max - x_min) + x_min\n",
    "    xyz_columns[:, 1] = xyz_columns[:, 1] * (y_max - y_min) + y_min\n",
    "    xyz_columns[:, 2] = xyz_columns[:, 2] * (z_max - z_min) + z_min\n",
    "\n",
    "    denormalized_array = np.concatenate((xyz_columns, normalized_array[:, 3:]), axis=1)\n",
    "    return denormalized_array\n",
    "\n",
    "def sample_pointcloud(model, num_samples, cond):\n",
    "    model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        pc_pr = (model.model.sample(num_samples, cond))\n",
    "        \n",
    "    return pc_pr\n",
    "\n",
    "def random_sample(data, sample_size):\n",
    "    \n",
    "    # Check if the sample size is greater than the number of points in the data\n",
    "    if sample_size > data.shape[0]:\n",
    "        raise ValueError(\"Sample size exceeds the number of points in the data\")\n",
    "\n",
    "    random_indices = np.random.choice(data.shape[0], sample_size, replace=False)\n",
    "    sampled_data = data[random_indices]\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "def save_checkpoint(model, optimizer, path, last_loss, min_valid_loss, epoch, wandb_run_id):\n",
    "        state = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'last_loss': last_loss.item(),\n",
    "            'epoch': epoch,\n",
    "            'min_valid_loss': min_valid_loss,\n",
    "            'wandb_run_id': wandb_run_id,\n",
    "        }\n",
    "\n",
    "        torch.save(state, path + '/model_' + str(epoch))\n",
    "        \n",
    "        \n",
    "def create_position_density_plots(x, y, z,\n",
    "                                  x_pr, y_pr, z_pr,\n",
    "                                  bins=100, t=1000, path=''):\n",
    "    \n",
    "    # Specify the number of bins for each axis\n",
    "    bins_x = np.linspace(min(x), max(x), bins)\n",
    "    bins_y = np.linspace(min(y), max(y), bins)\n",
    "    bins_z = np.linspace(min(z), max(z), bins)\n",
    "    \n",
    "    # Create subplots for each plane\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # XY Plane Ground Truth\n",
    "    plt.subplot(231)\n",
    "    plt.hist2d(x, y, bins=[bins_x, bins_y], cmap='Blues')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('XY Plane Ground Truth at t = {}'.format(t))\n",
    "    \n",
    "    # XZ Plane Ground Truth\n",
    "    plt.subplot(232)\n",
    "    plt.hist2d(x, z, bins=[bins_x, bins_z], cmap='Greens')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Z')\n",
    "    plt.title('XZ Plane Ground Truth at t = {}'.format(t))\n",
    "    \n",
    "    # YZ Plane Ground Truth\n",
    "    plt.subplot(233)\n",
    "    plt.hist2d(y, z, bins=[bins_y, bins_z], cmap='Reds')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('Y')\n",
    "    plt.ylabel('Z')\n",
    "    plt.title('YZ Plane Ground Truth at t = {}'.format(t))\n",
    "    \n",
    "    # XY Plane Prediction\n",
    "    plt.subplot(234)\n",
    "    plt.hist2d(x_pr, y_pr, bins=[bins_x, bins_y], cmap='Blues')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('XY Plane Prediction at t = {}'.format(t))\n",
    "    \n",
    "    # XZ Plane Prediction\n",
    "    plt.subplot(235)\n",
    "    plt.hist2d(x_pr, z_pr, bins=[bins_x, bins_z], cmap='Greens')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Z')\n",
    "    plt.title('XZ Plane Prediction at t = {}'.format(t))\n",
    "    \n",
    "    # YZ Plane Prediction\n",
    "    plt.subplot(236)\n",
    "    plt.hist2d(y_pr, z_pr, bins=[bins_y, bins_z], cmap='Reds')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('Y')\n",
    "    plt.ylabel('Z')\n",
    "    plt.title('YZ Plane Prediction at t = {}'.format(t))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plots as image files\n",
    "    if path:\n",
    "        plt.savefig(path + '/density_plots_{}.png'.format(t))\n",
    "\n",
    "def create_momentum_density_plots(px, py, pz,\n",
    "                                  px_pr, py_pr, pz_pr,\n",
    "                                  bins=100, t=1000, path=''):\n",
    "    \n",
    "    # Specify the number of bins for each axis\n",
    "    bins_px = np.linspace(min(px), max(px), bins)\n",
    "    bins_py = np.linspace(min(py), max(py), bins)\n",
    "    bins_pz = np.linspace(min(pz), max(pz), bins)\n",
    "    \n",
    "    # Create subplots for each plane\n",
    "    plt.figure(figsize=(15, 10)) \n",
    "    \n",
    "    # px-py Plane Ground Truth\n",
    "    plt.subplot(231)\n",
    "    plt.hist2d(px, py, bins=[bins_px, bins_py], cmap='Blues')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('px')\n",
    "    plt.ylabel('py')\n",
    "    plt.title('px-py Plane Ground Truth at t = {}'.format(t))\n",
    "    \n",
    "    # px-pz Plane Ground Truth\n",
    "    plt.subplot(232)\n",
    "    plt.hist2d(px, pz, bins=[bins_px, bins_pz], cmap='Greens')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('px')\n",
    "    plt.ylabel('pz')\n",
    "    plt.title('px-pz Plane Ground Truth at t = {}'.format(t))\n",
    "    \n",
    "    # py-pz Plane Ground Truth\n",
    "    plt.subplot(233)\n",
    "    plt.hist2d(py, pz, bins=[bins_py, bins_pz], cmap='Reds')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('py')\n",
    "    plt.ylabel('pz')\n",
    "    plt.title('py-pz Plane Ground Truth at t = {}'.format(t))\n",
    "    \n",
    "    # px-py Plane Prediction\n",
    "    plt.subplot(234)\n",
    "    plt.hist2d(px_pr, py_pr, bins=[bins_px, bins_py], cmap='Blues')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('px_pr')\n",
    "    plt.ylabel('py_pr')\n",
    "    plt.title('px-py Plane Prediction at t = {}'.format(t))\n",
    "    \n",
    "    # px-pz Plane Prediction\n",
    "    plt.subplot(235)\n",
    "    plt.hist2d(px_pr, pz_pr, bins=[bins_px, bins_pz], cmap='Greens')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('px_pr')\n",
    "    plt.ylabel('pz_pr')\n",
    "    plt.title('px-pz Plane Prediction at t = {}'.format(t))\n",
    "    \n",
    "    # py-pz Plane Prediction\n",
    "    plt.subplot(236)\n",
    "    plt.hist2d(py_pr, pz_pr, bins=[bins_py, bins_pz], cmap='Reds')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('py_pr')\n",
    "    plt.ylabel('pz_pr')\n",
    "    plt.title('py-pz Plane Prediction at t = {}'.format(t))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plots as image files\n",
    "    if path:\n",
    "        plt.savefig(path + '/momentum_density_plots_{}.png'.format(t))\n",
    "\n",
    "\n",
    "\n",
    "def create_force_density_plots(fx, fy, fz,\n",
    "                               fx_pr, fy_pr, fz_pr,\n",
    "                               bins=100, t=1000, path=''):\n",
    "    \n",
    "    # Specify the number of bins for each axis\n",
    "    bins_fx = np.linspace(min(fx), max(fx), bins)\n",
    "    bins_fy = np.linspace(min(fy), max(fy), bins)\n",
    "    bins_fz = np.linspace(min(fz), max(fz), bins)\n",
    "    \n",
    "    # Create subplots for each plane\n",
    "    plt.figure(figsize=(15, 10))  # Adjust the figure size\n",
    "    \n",
    "    # fx-fy Plane Ground Truth\n",
    "    plt.subplot(231)\n",
    "    plt.hist2d(fx, fy, bins=[bins_fx, bins_fy], cmap='Blues')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('fx')\n",
    "    plt.ylabel('fy')\n",
    "    plt.title('fx-fy Plane Ground Truth at t = {}'.format(t))\n",
    "    \n",
    "    # fx-fz Plane Ground Truth\n",
    "    plt.subplot(232)\n",
    "    plt.hist2d(fx, fz, bins=[bins_fx, bins_fz], cmap='Greens')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('fx')\n",
    "    plt.ylabel('fz')\n",
    "    plt.title('fx-fz Plane Ground Truth at t = {}'.format(t))\n",
    "    \n",
    "    # fy-fz Plane Ground Truth\n",
    "    plt.subplot(233)\n",
    "    plt.hist2d(fy, fz, bins=[bins_fy, bins_fz], cmap='Reds')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('fy')\n",
    "    plt.ylabel('fz')\n",
    "    plt.title('fy-fz Plane Ground Truth at t = {}'.format(t))\n",
    "    \n",
    "    # fx-fy Plane Prediction\n",
    "    plt.subplot(234)\n",
    "    plt.hist2d(fx_pr, fy_pr, bins=[bins_fx, bins_fy], cmap='Blues')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('fx_pr')\n",
    "    plt.ylabel('fy_pr')\n",
    "    plt.title('fx-fy Plane Prediction at t = {}'.format(t))\n",
    "    \n",
    "    # fx-fz Plane Prediction\n",
    "    plt.subplot(235)\n",
    "    plt.hist2d(fx_pr, fz_pr, bins=[bins_fx, bins_fz], cmap='Greens')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('fx_pr')\n",
    "    plt.ylabel('fz_pr')\n",
    "    plt.title('fx-fz Plane Prediction at t = {}'.format(t))\n",
    "    \n",
    "    # fy-fz Plane Prediction\n",
    "    plt.subplot(236)\n",
    "    plt.hist2d(fy_pr, fz_pr, bins=[bins_fy, bins_fz], cmap='Reds')\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('fy_pr')\n",
    "    plt.ylabel('fz_pr')\n",
    "    plt.title('fy-fz Plane Prediction at t = {}'.format(t))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plots as image files\n",
    "    if path:\n",
    "        plt.savefig(path + '/force_density_plots_{}.png'.format(t))\n",
    "    \n",
    "\n",
    "def inference(gpu_index,t_index):\n",
    "\n",
    "    p_gt = np.load(hyperparameter_defaults[\"pathpattern1\"].format(t_index),allow_pickle = True)\n",
    "\n",
    "    p_gt = [random_sample(element, sample_size=10000) for element in p_gt]\n",
    "    p_gt = np.array(p_gt, dtype = np.float32)\n",
    "\n",
    "    p_rad = torch.from_numpy(np.load(hyperparameter_defaults[\"pathpattern2\"].format(t_index)).astype(np.cfloat))\n",
    "\n",
    "    p_rad_x = p_rad[gpu_index,0,:]\n",
    "    p_rad_y = p_rad[gpu_index,1,:]\n",
    "    p_rad_z = p_rad[gpu_index,2,:]\n",
    "\n",
    "    p_rad = p_rad[:, 1:, :]\n",
    "    p_rad = p_rad.view(p_rad.shape[0], -1)\n",
    "    p_rad = p_rad.unsqueeze(1)\n",
    "\n",
    "    p_rad = p_rad[gpu_index,:,:]\n",
    "    p_gt = p_gt[gpu_index,:,:]\n",
    "\n",
    "    # Compute the phase (angle) of the complex number\n",
    "    phase = torch.angle(p_rad)\n",
    "\n",
    "    # Compute the amplitude (magnitude) of the complex number\n",
    "    amplitude = torch.abs(p_rad)\n",
    "    p_rad = torch.cat((amplitude, phase), dim=1).to(torch.float32)\n",
    "\n",
    "    num_samples = 1\n",
    "    cond = p_rad.cuda()\n",
    "\n",
    "    pc_pr =  sample_pointcloud(model, num_samples, cond)\n",
    "\n",
    "    pc_pr = pc_pr.squeeze().cpu().numpy()\n",
    "\n",
    "    pc_pr = pc_pr.reshape(10000,9)\n",
    "\n",
    "    pc_pr = denormalize_columns(pc_pr, p_gt)\n",
    "\n",
    "    x = p_gt[:, 0]  # X coordinates\n",
    "    y = p_gt[:, 1]  # Y coordinates\n",
    "    z = p_gt[:, 2]  # Z coordinates\n",
    "\n",
    "    px = p_gt[:, 3]  # Px component of momentum\n",
    "    py = p_gt[:, 4]  # Py component of momentum\n",
    "    pz = p_gt[:, 5]  # Pz component of momentum\n",
    "\n",
    "    fx = p_gt[:, 6]  # Fx component of force\n",
    "    fy = p_gt[:, 7]  # Fy component of force\n",
    "    fz = p_gt[:, 8]  # Fz component of force\n",
    "\n",
    "\n",
    "    x_pr = pc_pr[:, 0]  # X coordinates\n",
    "    y_pr = pc_pr[:, 1]  # Y coordinates\n",
    "    z_pr = pc_pr[:, 2]  # Z coordinates\n",
    "\n",
    "    px_pr = pc_pr[:, 3]  # Px component of momentum\n",
    "    py_pr = pc_pr[:, 4]  # Py component of momentum\n",
    "    pz_pr = pc_pr[:, 5]  # Pz component of momentum\n",
    "\n",
    "    fx_pr = pc_pr[:, 6]  # Fx component of force\n",
    "    fy_pr = pc_pr[:, 7]  # Fy component of force\n",
    "    fz_pr = pc_pr[:, 8]  # Fz component of force\n",
    "\n",
    "\n",
    "    create_position_density_plots(x, y, z, x_pr, y_pr, z_pr, bins=100, t=t_index)\n",
    "\n",
    "    create_momentum_density_plots(px, py, pz, px_pr, py_pr, pz_pr, bins=100, t=t_index)\n",
    "\n",
    "    create_force_density_plots(fx, fy, fz, fx_pr, fy_pr, fz_pr, bins=100, t=t_index)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9a6c7c-56b2-41a3-aafc-c507dfeb6db8",
   "metadata": {},
   "source": [
    "## Define the loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d77efc-cfe4-4f97-a103-26171f522bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self, pathpattern1=\"/bigdata/hplsim/aipp/Jeyhun/khi/particle_box/40_80_80_160_0_2/{}.npy\", pathpattern2=\"/bigdata/hplsim/aipp/Jeyhun/khi/part_rad/radiation_ex/{}.npy\", t0=0, t1=100, timebatchsize=20, particlebatchsize=10240):\n",
    "        self.pathpattern1 = pathpattern1\n",
    "        self.pathpattern2 = pathpattern2\n",
    "                \n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        \n",
    "        self.timebatchsize = timebatchsize\n",
    "        self.particlebatchsize = particlebatchsize\n",
    "\n",
    "        num_files = t1 - t0\n",
    "        missing_files = [i for i in range(t0, t1) if not os.path.exists(pathpattern1.format(i))]\n",
    "        num_missing = len(missing_files)\n",
    "        all_files_exist = num_missing == 0\n",
    "\n",
    "        if all_files_exist:\n",
    "            print(\"All {} files from {} to {} exist in the directory.\".format(num_files, t0, t1))\n",
    "        else:\n",
    "            print(\"{} files are missing out of {} in the directory.\".format(num_missing, num_files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.t1 - self.t0\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        class Epoch:\n",
    "            def __init__(self, loader, t0, t1, timebatchsize=20, particlebatchsize=10240):\n",
    "                self.perm = torch.randperm(len(loader))\n",
    "                self.loader = loader\n",
    "                self.t0 = t0\n",
    "                self.t1 = t1\n",
    "                self.timebatchsize = timebatchsize\n",
    "                self.particlebatchsize = particlebatchsize\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.loader) // self.timebatchsize\n",
    "        \n",
    "            def __getitem__(self, timebatch):\n",
    "                i = self.timebatchsize*timebatch\n",
    "                bi = self.perm[i:i+self.timebatchsize]\n",
    "                radiation = []\n",
    "                particles = []\n",
    "                for time in bi:\n",
    "                    index = time + self.t0\n",
    "                    \n",
    "                    p = np.load(self.loader.pathpattern1.format(index), allow_pickle = True)\n",
    "                    \n",
    "                    p = [normalize_columns(element) for element in p]\n",
    "                    p = np.array(p, dtype=object)\n",
    "                    \n",
    "                    p = [random_sample(element, sample_size=10000) for element in p]\n",
    "                    p = torch.from_numpy(np.array(p, dtype = np.float32))\n",
    "                    \n",
    "                    p = p.view(p.shape[0],-1)\n",
    "                    \n",
    "                    r = torch.from_numpy(np.load(self.loader.pathpattern2.format(index)).astype(np.cfloat) )\n",
    "                    r = r[:, 1:, :]\n",
    "                    r = r.view(r.shape[0], -1)\n",
    "                    \n",
    "                    # Compute the phase (angle) of the complex number\n",
    "                    phase = torch.angle(r)\n",
    "                    \n",
    "                    # Compute the absolute value of the complex number\n",
    "                    absolute = torch.abs(r)\n",
    "                    r = torch.cat((absolute, phase), dim=1).to(torch.float32)\n",
    "\n",
    "                    particles.append(p)\n",
    "                    radiation.append(r)\n",
    "                \n",
    "                particles = torch.cat(particles)\n",
    "                radiation = torch.cat(radiation)\n",
    "                \n",
    "                class Timebatch:\n",
    "                    def __init__(self, particles, radiation, batchsize):\n",
    "                        self.batchsize = batchsize\n",
    "                        self.particles = particles\n",
    "                        self.radiation = radiation\n",
    "                        \n",
    "                        self.perm = torch.randperm(self.radiation.shape[0])\n",
    "                        \n",
    "                    def __len__(self):\n",
    "                        return self.radiation.shape[0] // self.batchsize\n",
    "\n",
    "                    def __getitem__(self, batch):\n",
    "                        i = self.batchsize*batch\n",
    "                        bi = self.perm[i:i+self.batchsize]\n",
    "                    \n",
    "                        return self.particles[bi], self.radiation[bi]\n",
    "                \n",
    "                return Timebatch(particles, radiation, self.particlebatchsize)\n",
    "                    \n",
    "        return Epoch(self, self.t0, self.t1, self.timebatchsize, self.particlebatchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007bd426-f778-4651-ae21-b787f59cf852",
   "metadata": {},
   "source": [
    "## Set the hyperparameters of the model and initialise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb389ac7-25c5-484b-8f2f-dd91e20175c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "t0 = 1990,\n",
    "t1 = 2001,\n",
    "dim_input = 90000,\n",
    "timebatchsize = 4,\n",
    "particlebatchsize = 32,\n",
    "dim_condition = 2048,\n",
    "num_coupling_layers = 3,\n",
    "hidden_size = 64,\n",
    "lr = 0.001,\n",
    "num_epochs = 10,\n",
    "num_blocks_mat = 2,\n",
    "activation = 'gelu',\n",
    "pathpattern1 = \"/bigdata/hplsim/aipp/Jeyhun/khi/part_rad/particle_002/{}.npy\",\n",
    "pathpattern2 = \"/bigdata/hplsim/aipp/Jeyhun/khi/part_rad/radiation_ex_002/{}.npy\"\n",
    ")\n",
    "\n",
    "enable_wandb = False\n",
    "start_epoch = 0\n",
    "min_valid_loss = np.inf\n",
    "\n",
    "if enable_wandb:\n",
    "    print('New session...')\n",
    "    # Pass your defaults to wandb.init\n",
    "    wandb.init(entity=\"jeyhun\", config=hyperparameter_defaults, project=\"khi_public\")\n",
    "\n",
    "    # Access all hyperparameter values through wandb.config\n",
    "    config = wandb.config\n",
    "\n",
    "l = Loader(pathpattern1 = hyperparameter_defaults[\"pathpattern1\"],\n",
    "           pathpattern2 = hyperparameter_defaults[\"pathpattern2\"],\n",
    "           t0 = hyperparameter_defaults[\"t0\"],\n",
    "           t1 = hyperparameter_defaults[\"t1\"],\n",
    "           timebatchsize = hyperparameter_defaults[\"timebatchsize\"],\n",
    "           particlebatchsize = hyperparameter_defaults[\"particlebatchsize\"])\n",
    "\n",
    "model = (PC_MAF(dim_condition = hyperparameter_defaults[\"dim_condition\"],\n",
    "                           dim_input = hyperparameter_defaults[\"dim_input\"],\n",
    "                           num_coupling_layers = hyperparameter_defaults[\"num_coupling_layers\"],\n",
    "                           hidden_size = hyperparameter_defaults[\"hidden_size\"],\n",
    "                           device = 'cuda',\n",
    "                           num_blocks_mat = hyperparameter_defaults[\"num_blocks_mat\"],\n",
    "                           activation = hyperparameter_defaults[\"activation\"]\n",
    "                         ))\n",
    "\n",
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparameter_defaults[\"lr\"])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.9)\n",
    "\n",
    "if enable_wandb:\n",
    "    directory ='/bigdata/hplsim/aipp/Jeyhun/khi/checkpoints/'+str(wandb.run.id)\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory '{directory}' created.\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61a157-198c-47dd-b9e0-ce0cd34154df",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a8683-1df0-492d-9fc8-d4deb6940eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = l[0]\n",
    "\n",
    "start_time = time.time()\n",
    "for i_epoch in range(start_epoch, hyperparameter_defaults[\"num_epochs\"]):   \n",
    "    loss_overall = []\n",
    "    for tb in range(len(epoch)):\n",
    "        loss_avg = []\n",
    "        timebatch = epoch[tb]\n",
    "\n",
    "        start_timebatch = time.time()\n",
    "        for b in range(len(timebatch)):\n",
    "            optimizer.zero_grad()\n",
    "            phase_space, radiation = timebatch[b]\n",
    "\n",
    "            loss = - model.model.log_prob(inputs=phase_space.to(model.device),context=radiation.to(model.device))\n",
    "\n",
    "            loss = loss.mean()\n",
    "            loss_avg.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        end_timebatch = time.time()\n",
    "        elapsed_timebatch = end_timebatch - start_timebatch\n",
    "\n",
    "        loss_timebatch_avg = sum(loss_avg)/len(loss_avg)\n",
    "        loss_overall.append(loss_timebatch_avg)\n",
    "        print('i_epoch:{}, tb: {}, last timebatch loss: {}, avg_loss: {}, time: {}'.format(i_epoch,tb,loss.item(), loss_timebatch_avg, elapsed_timebatch))\n",
    "\n",
    "    loss_overall_avg = sum(loss_overall)/len(loss_overall)  \n",
    "\n",
    "    if min_valid_loss > loss_overall_avg:     \n",
    "        print(f'Training Loss Decreased({min_valid_loss:.6f}--->{loss_overall_avg:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = loss_overall_avg\n",
    "        # Saving State Dict\n",
    "        # torch.save(model.state_dict(), directory + '/best_model_', _use_new_zipfile_serialization=False)\n",
    "\n",
    "    if (i_epoch + 1) % 10 == 0 and enable_wandb:\n",
    "        save_checkpoint(model, optimizer, directory, loss, min_valid_loss, i_epoch, wandb.run.id)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if enable_wandb:\n",
    "        # Log the loss and accuracy values at the end of each epoch\n",
    "        wandb.log({\n",
    "            \"Epoch\": i_epoch,\n",
    "            \"loss_timebatch_avg_loss\": loss_timebatch_avg,\n",
    "            \"loss_overall_avg\": loss_overall_avg,\n",
    "            \"min_valid_loss\": min_valid_loss,\n",
    "        })\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Total elapsed time: {elapsed_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53b61bb-c43f-43f7-95ac-ff25817119f8",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da59b0-1027-4aff-b798-cf0ba6ae62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the time step and gpu box you want to visualise\n",
    "t_index = 2000\n",
    "gpu_index = 19\n",
    "\n",
    "inference(gpu_index,t_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
